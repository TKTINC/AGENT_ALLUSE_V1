"""
ALL-USE Learning Systems - Real-Time Analytics Engine

This module implements a real-time analytics engine for the ALL-USE Learning Systems,
providing immediate insights and analysis of streaming performance data.

The real-time analytics engine is designed to:
- Process streaming data with minimal latency
- Detect anomalies and patterns in real-time
- Generate alerts and notifications
- Provide real-time dashboards and visualizations
- Support complex event processing

Classes:
- RealTimeAnalyticsEngine: Core real-time analytics engine
- StreamProcessor: Processes streaming data
- AnomalyDetector: Detects anomalies in real-time
- PatternMatcher: Identifies patterns in streaming data
- AlertManager: Manages alerts and notifications

Version: 1.0.0
"""

import time
import logging
import threading
import queue
import numpy as np
import json
from typing import Dict, List, Any, Optional, Callable, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import uuid
import statistics
from collections import deque, defaultdict
import math

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AnalyticsType(Enum):
    """Types of analytics that can be performed."""
    TREND_ANALYSIS = 1
    ANOMALY_DETECTION = 2
    PATTERN_RECOGNITION = 3
    STATISTICAL_ANALYSIS = 4
    CORRELATION_ANALYSIS = 5
    FORECASTING = 6

class AlertSeverity(Enum):
    """Severity levels for alerts."""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    INFO = 5

@dataclass
class AnalyticsConfig:
    """Configuration for the analytics engine."""
    window_size: int = 100  # Number of data points to analyze
    anomaly_threshold: float = 2.0  # Standard deviations for anomaly detection
    trend_threshold: float = 0.1  # Minimum trend strength
    alert_cooldown: int = 300  # Seconds between similar alerts
    enable_forecasting: bool = True
    enable_pattern_matching: bool = True
    enable_anomaly_detection: bool = True

@dataclass
class DataPoint:
    """Represents a single data point for analysis."""
    timestamp: float
    value: float
    metric_name: str
    tags: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AnalyticsResult:
    """Result of an analytics operation."""
    analytics_type: AnalyticsType
    metric_name: str
    timestamp: float
    result: Dict[str, Any]
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Alert:
    """Represents an alert generated by the analytics engine."""
    alert_id: str
    severity: AlertSeverity
    message: str
    metric_name: str
    timestamp: float
    value: float
    threshold: float
    metadata: Dict[str, Any] = field(default_factory=dict)

class StreamProcessor:
    """Processes streaming data for real-time analytics."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.data_windows = defaultdict(lambda: deque(maxlen=config.window_size))
        self.lock = threading.Lock()
        
    def add_data_point(self, data_point: DataPoint) -> None:
        """Add a data point to the processing window."""
        with self.lock:
            self.data_windows[data_point.metric_name].append(data_point)
            
    def get_window_data(self, metric_name: str) -> List[DataPoint]:
        """Get the current window data for a metric."""
        with self.lock:
            return list(self.data_windows[metric_name])
            
    def get_window_values(self, metric_name: str) -> List[float]:
        """Get just the values from the current window."""
        with self.lock:
            return [dp.value for dp in self.data_windows[metric_name]]
            
    def calculate_statistics(self, metric_name: str) -> Dict[str, float]:
        """Calculate basic statistics for a metric."""
        values = self.get_window_values(metric_name)
        if not values:
            return {}
            
        return {
            'count': len(values),
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'std_dev': statistics.stdev(values) if len(values) > 1 else 0.0,
            'min': min(values),
            'max': max(values),
            'range': max(values) - min(values),
            'variance': statistics.variance(values) if len(values) > 1 else 0.0
        }

class AnomalyDetector:
    """Detects anomalies in streaming data."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.baseline_stats = {}
        
    def update_baseline(self, metric_name: str, values: List[float]) -> None:
        """Update the baseline statistics for anomaly detection."""
        if len(values) < 2:
            return
            
        self.baseline_stats[metric_name] = {
            'mean': statistics.mean(values),
            'std_dev': statistics.stdev(values),
            'updated_at': time.time()
        }
        
    def detect_anomaly(self, data_point: DataPoint) -> Optional[Alert]:
        """Detect if a data point is anomalous."""
        metric_name = data_point.metric_name
        
        if metric_name not in self.baseline_stats:
            return None
            
        baseline = self.baseline_stats[metric_name]
        z_score = abs(data_point.value - baseline['mean']) / baseline['std_dev'] if baseline['std_dev'] > 0 else 0
        
        if z_score > self.config.anomaly_threshold:
            severity = AlertSeverity.CRITICAL if z_score > 3.0 else AlertSeverity.HIGH
            
            return Alert(
                alert_id=str(uuid.uuid4()),
                severity=severity,
                message=f"Anomaly detected in {metric_name}: value {data_point.value:.2f} (z-score: {z_score:.2f})",
                metric_name=metric_name,
                timestamp=data_point.timestamp,
                value=data_point.value,
                threshold=baseline['mean'] + (self.config.anomaly_threshold * baseline['std_dev']),
                metadata={
                    'z_score': z_score,
                    'baseline_mean': baseline['mean'],
                    'baseline_std_dev': baseline['std_dev']
                }
            )
        
        return None

class TrendAnalyzer:
    """Analyzes trends in streaming data."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        
    def analyze_trend(self, values: List[float], timestamps: List[float]) -> Dict[str, Any]:
        """Analyze trend in a series of values."""
        if len(values) < 3:
            return {'trend': 'insufficient_data'}
            
        # Calculate linear regression
        n = len(values)
        x = np.array(range(n))
        y = np.array(values)
        
        # Calculate slope and correlation
        slope = np.polyfit(x, y, 1)[0]
        correlation = np.corrcoef(x, y)[0, 1] if n > 1 else 0
        
        # Determine trend direction and strength
        trend_strength = abs(correlation)
        
        if trend_strength < self.config.trend_threshold:
            trend_direction = 'stable'
        elif slope > 0:
            trend_direction = 'increasing'
        else:
            trend_direction = 'decreasing'
            
        # Calculate rate of change
        if len(timestamps) >= 2:
            time_diff = timestamps[-1] - timestamps[0]
            value_diff = values[-1] - values[0]
            rate_of_change = value_diff / time_diff if time_diff > 0 else 0
        else:
            rate_of_change = 0
            
        return {
            'trend': trend_direction,
            'strength': trend_strength,
            'slope': slope,
            'correlation': correlation,
            'rate_of_change': rate_of_change,
            'confidence': min(trend_strength * 2, 1.0)  # Scale to 0-1
        }

class PatternMatcher:
    """Identifies patterns in streaming data."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.known_patterns = {}
        
    def detect_patterns(self, values: List[float]) -> List[Dict[str, Any]]:
        """Detect patterns in a series of values."""
        patterns = []
        
        if len(values) < 5:
            return patterns
            
        # Detect cyclical patterns
        cyclical_pattern = self._detect_cyclical_pattern(values)
        if cyclical_pattern:
            patterns.append(cyclical_pattern)
            
        # Detect spike patterns
        spike_pattern = self._detect_spike_pattern(values)
        if spike_pattern:
            patterns.append(spike_pattern)
            
        # Detect step changes
        step_pattern = self._detect_step_change(values)
        if step_pattern:
            patterns.append(step_pattern)
            
        return patterns
        
    def _detect_cyclical_pattern(self, values: List[float]) -> Optional[Dict[str, Any]]:
        """Detect cyclical patterns using autocorrelation."""
        if len(values) < 10:
            return None
            
        # Simple autocorrelation for cycle detection
        autocorr = np.correlate(values, values, mode='full')
        autocorr = autocorr[autocorr.size // 2:]
        
        # Find peaks in autocorrelation
        peaks = []
        for i in range(1, len(autocorr) - 1):
            if autocorr[i] > autocorr[i-1] and autocorr[i] > autocorr[i+1]:
                peaks.append((i, autocorr[i]))
                
        if peaks and len(peaks) > 1:
            # Find the most significant peak (excluding the first one at lag 0)
            significant_peaks = [p for p in peaks if p[0] > 1 and p[1] > 0.5]
            if significant_peaks:
                cycle_length = significant_peaks[0][0]
                confidence = significant_peaks[0][1]
                
                return {
                    'type': 'cyclical',
                    'cycle_length': cycle_length,
                    'confidence': min(confidence, 1.0)
                }
                
        return None
        
    def _detect_spike_pattern(self, values: List[float]) -> Optional[Dict[str, Any]]:
        """Detect spike patterns in the data."""
        if len(values) < 5:
            return None
            
        mean_val = statistics.mean(values)
        std_val = statistics.stdev(values) if len(values) > 1 else 0
        
        spikes = []
        for i, val in enumerate(values):
            if std_val > 0 and abs(val - mean_val) > 2 * std_val:
                spikes.append((i, val))
                
        if len(spikes) > 0:
            return {
                'type': 'spike',
                'spike_count': len(spikes),
                'spike_positions': [s[0] for s in spikes],
                'spike_values': [s[1] for s in spikes],
                'confidence': min(len(spikes) / len(values) * 5, 1.0)
            }
            
        return None
        
    def _detect_step_change(self, values: List[float]) -> Optional[Dict[str, Any]]:
        """Detect step changes in the data."""
        if len(values) < 6:
            return None
            
        # Look for significant changes between consecutive segments
        mid_point = len(values) // 2
        first_half = values[:mid_point]
        second_half = values[mid_point:]
        
        if len(first_half) > 1 and len(second_half) > 1:
            mean1 = statistics.mean(first_half)
            mean2 = statistics.mean(second_half)
            std1 = statistics.stdev(first_half)
            std2 = statistics.stdev(second_half)
            
            # Calculate the magnitude of the step change
            step_magnitude = abs(mean2 - mean1)
            pooled_std = math.sqrt((std1**2 + std2**2) / 2) if std1 > 0 and std2 > 0 else 0
            
            if pooled_std > 0 and step_magnitude > 1.5 * pooled_std:
                return {
                    'type': 'step_change',
                    'step_position': mid_point,
                    'step_magnitude': step_magnitude,
                    'direction': 'up' if mean2 > mean1 else 'down',
                    'confidence': min(step_magnitude / pooled_std / 3, 1.0)
                }
                
        return None

class AlertManager:
    """Manages alerts and notifications."""
    
    def __init__(self, config: AnalyticsConfig):
        self.config = config
        self.active_alerts = {}
        self.alert_history = []
        self.last_alert_times = defaultdict(float)
        self.lock = threading.Lock()
        
    def process_alert(self, alert: Alert) -> bool:
        """Process an alert and determine if it should be sent."""
        with self.lock:
            # Check cooldown period
            alert_key = f"{alert.metric_name}_{alert.severity.name}"
            current_time = time.time()
            
            if current_time - self.last_alert_times[alert_key] < self.config.alert_cooldown:
                return False  # Still in cooldown period
                
            # Add to active alerts
            self.active_alerts[alert.alert_id] = alert
            self.alert_history.append(alert)
            self.last_alert_times[alert_key] = current_time
            
            logger.warning(f"Alert generated: {alert.message}")
            return True
            
    def get_active_alerts(self) -> List[Alert]:
        """Get all active alerts."""
        with self.lock:
            return list(self.active_alerts.values())
            
    def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an active alert."""
        with self.lock:
            if alert_id in self.active_alerts:
                del self.active_alerts[alert_id]
                return True
            return False
            
    def get_alert_summary(self) -> Dict[str, Any]:
        """Get a summary of alert activity."""
        with self.lock:
            severity_counts = defaultdict(int)
            for alert in self.active_alerts.values():
                severity_counts[alert.severity.name] += 1
                
            return {
                'active_alerts': len(self.active_alerts),
                'total_alerts_generated': len(self.alert_history),
                'severity_breakdown': dict(severity_counts),
                'last_alert_time': max([a.timestamp for a in self.active_alerts.values()]) if self.active_alerts else 0
            }

class RealTimeAnalyticsEngine:
    """Main real-time analytics engine that coordinates all analytics components."""
    
    def __init__(self, config: Optional[AnalyticsConfig] = None):
        self.config = config or AnalyticsConfig()
        self.stream_processor = StreamProcessor(self.config)
        self.anomaly_detector = AnomalyDetector(self.config)
        self.trend_analyzer = TrendAnalyzer(self.config)
        self.pattern_matcher = PatternMatcher(self.config)
        self.alert_manager = AlertManager(self.config)
        
        self.analytics_results = []
        self.is_running = False
        self.processing_thread = None
        self.lock = threading.Lock()
        
        logger.info("Real-time analytics engine initialized")
        
    def start(self) -> None:
        """Start the analytics engine."""
        if self.is_running:
            return
            
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
        self.processing_thread.start()
        
        logger.info("Real-time analytics engine started")
        
    def stop(self) -> None:
        """Stop the analytics engine."""
        self.is_running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=5.0)
            
        logger.info("Real-time analytics engine stopped")
        
    def add_data_point(self, data_point: DataPoint) -> None:
        """Add a data point for analysis."""
        self.stream_processor.add_data_point(data_point)
        
        # Check for immediate anomalies
        if self.config.enable_anomaly_detection:
            alert = self.anomaly_detector.detect_anomaly(data_point)
            if alert:
                self.alert_manager.process_alert(alert)
                
    def get_analytics_results(self, metric_name: Optional[str] = None, 
                            analytics_type: Optional[AnalyticsType] = None) -> List[AnalyticsResult]:
        """Get analytics results with optional filtering."""
        with self.lock:
            results = self.analytics_results.copy()
            
        if metric_name:
            results = [r for r in results if r.metric_name == metric_name]
            
        if analytics_type:
            results = [r for r in results if r.analytics_type == analytics_type]
            
        return results
        
    def get_real_time_dashboard(self) -> Dict[str, Any]:
        """Get real-time dashboard data."""
        dashboard = {
            'timestamp': time.time(),
            'metrics_summary': {},
            'active_alerts': self.alert_manager.get_active_alerts(),
            'alert_summary': self.alert_manager.get_alert_summary(),
            'analytics_summary': {
                'total_results': len(self.analytics_results),
                'recent_results': len([r for r in self.analytics_results if time.time() - r.timestamp < 300])
            }
        }
        
        # Add metrics summary
        for metric_name in self.stream_processor.data_windows.keys():
            stats = self.stream_processor.calculate_statistics(metric_name)
            if stats:
                dashboard['metrics_summary'][metric_name] = stats
                
        return dashboard
        
    def _processing_loop(self) -> None:
        """Main processing loop for analytics."""
        while self.is_running:
            try:
                self._process_analytics()
                time.sleep(1.0)  # Process every second
            except Exception as e:
                logger.error(f"Error in analytics processing loop: {e}")
                time.sleep(5.0)  # Wait longer on error
                
    def _process_analytics(self) -> None:
        """Process analytics for all metrics."""
        current_time = time.time()
        
        for metric_name in list(self.stream_processor.data_windows.keys()):
            try:
                self._process_metric_analytics(metric_name, current_time)
            except Exception as e:
                logger.error(f"Error processing analytics for metric {metric_name}: {e}")
                
    def _process_metric_analytics(self, metric_name: str, current_time: float) -> None:
        """Process analytics for a specific metric."""
        window_data = self.stream_processor.get_window_data(metric_name)
        if len(window_data) < 3:
            return
            
        values = [dp.value for dp in window_data]
        timestamps = [dp.timestamp for dp in window_data]
        
        # Update anomaly detection baseline
        if self.config.enable_anomaly_detection:
            self.anomaly_detector.update_baseline(metric_name, values)
            
        # Perform trend analysis
        trend_result = self.trend_analyzer.analyze_trend(values, timestamps)
        if trend_result['trend'] != 'insufficient_data':
            analytics_result = AnalyticsResult(
                analytics_type=AnalyticsType.TREND_ANALYSIS,
                metric_name=metric_name,
                timestamp=current_time,
                result=trend_result,
                confidence=trend_result.get('confidence', 0.5)
            )
            
            with self.lock:
                self.analytics_results.append(analytics_result)
                
        # Perform pattern matching
        if self.config.enable_pattern_matching:
            patterns = self.pattern_matcher.detect_patterns(values)
            for pattern in patterns:
                analytics_result = AnalyticsResult(
                    analytics_type=AnalyticsType.PATTERN_RECOGNITION,
                    metric_name=metric_name,
                    timestamp=current_time,
                    result=pattern,
                    confidence=pattern.get('confidence', 0.5)
                )
                
                with self.lock:
                    self.analytics_results.append(analytics_result)
                    
        # Clean up old results (keep only last 1000)
        with self.lock:
            if len(self.analytics_results) > 1000:
                self.analytics_results = self.analytics_results[-1000:]

# Example usage and testing
if __name__ == "__main__":
    # Create analytics engine
    config = AnalyticsConfig(
        window_size=50,
        anomaly_threshold=2.0,
        trend_threshold=0.1,
        enable_forecasting=True,
        enable_pattern_matching=True,
        enable_anomaly_detection=True
    )
    
    engine = RealTimeAnalyticsEngine(config)
    engine.start()
    
    # Simulate some data
    import random
    
    try:
        for i in range(100):
            # Generate some test data with trends and anomalies
            base_value = 100 + i * 0.5  # Upward trend
            noise = random.gauss(0, 5)
            
            # Add occasional anomalies
            if random.random() < 0.05:
                noise += random.choice([-30, 30])
                
            value = base_value + noise
            
            data_point = DataPoint(
                timestamp=time.time(),
                value=value,
                metric_name="test_metric",
                tags={"source": "test"}
            )
            
            engine.add_data_point(data_point)
            time.sleep(0.1)
            
        # Get dashboard
        dashboard = engine.get_real_time_dashboard()
        print(f"Dashboard: {json.dumps(dashboard, indent=2, default=str)}")
        
        # Get analytics results
        results = engine.get_analytics_results("test_metric")
        print(f"Analytics results: {len(results)} results generated")
        
        for result in results[-5:]:  # Show last 5 results
            print(f"  {result.analytics_type.name}: {result.result}")
            
    finally:
        engine.stop()

